#include "{{solver_name}}_solver.h"
#include <time.h>

class DengoSolver{

    private:
        {{solver_name}}_data rate_data;
        void*    cvode_mem;
        cusparseHandle_t   cusp_handle;
        cusolverSpHandle_t cusol_handle;
        realtype *ydata, *abstol_data;
        N_Vector y, abstol;
        SUNMatrix A;
        SUNLinearSolver LS;
        unsigned long d;
        unsigned long neq;
        int initialize_cvode_solver();

    public:
        DengoSolver()
        {
            neq = BATCHSIZE*nchem;
            int flag = initialize_cvode_solver();
        }

        ~DengoSolver()
        {
            /* Print some final statistics */
            PrintFinalStats(cvode_mem, LS);
            /* Free y and abstol vectors */
            N_VDestroy(y);
            N_VDestroy(abstol);
            /* Free integrator memory */
            CVodeFree(&cvode_mem);
            /* Free the linear solver memory */
            SUNLinSolFree(LS);
            /* Free the matrix memory */
            SUNMatDestroy(A);
            /* Destroy the cuSOLVER and cuSPARSE handles */
            cusparseDestroy(cusp_handle);
            cusolverSpDestroy(cusol_handle);
        };

        int EvolveChemistry(double dtf, double z, double *input, double *rtol, double *atol, unsigned long dims);

};

int DengoSolver::initialize_cvode_solver(){
    rate_data = {{solver_name}}_setup_data(NULL, NULL);
    {{solver_name}}_read_cooling_tables( &rate_data);
    {{solver_name}}_read_rate_tables( &rate_data);

    int retval, iout;
    // create CUDA vector
    y = abstol = NULL;
    A = NULL;
    LS = NULL;
    cvode_mem = NULL;

    neq = BATCHSIZE*nchem;

    // Initialize cuSolver and suSPARSE
    cusparseCreate  (&cusp_handle);
    cusolverSpCreate(&cusol_handle);

    /////////////////////////////////////////////////////////////////
    // Initialize memory spaces for cvode solver //
    /////////////////////////////////////////////////////////////////
    // Create CUDA vector of length neq for I.C. and abstol
    y = N_VNew_Cuda(neq);
    if (check_retval ((void *)y, "N_VNew_Cuda", 0)) return(1);
    abstol = N_VNew_Cuda(neq);
    if (check_retval ((void *)abstol, "N_VNew_Cuda", 0)) return(1);

    ydata       = N_VGetHostArrayPointer_Cuda(y);
    abstol_data = N_VGetHostArrayPointer_Cuda(abstol);
    // Initialize cuda vector
    for (unsigned long i = 0; i < neq; i++){
        ydata[i] = 1.0;
        abstol_data[i] = 1.0e-6;
    }
    N_VCopyToDevice_Cuda(y);
    N_VCopyToDevice_Cuda(abstol);

    /* Call CVodeCreate to create the solver memory and specify the
     * Backward Differentiation Formula */
    cvode_mem = CVodeCreate(CV_BDF);
    if (check_retval((void *)cvode_mem, "CVodeCreate", 0)) return(1);

    /* Call CVodeInit to initialize the integrator memory and specify the
     * user's right hand side function in y'=f(t,y), the inital time T0, and
     * the initial dependent variable vector y. */
    retval = CVodeInit(cvode_mem, calculate_rhs_{{solver_name}}, 0.0, y);
    if (check_retval(&retval, "CVodeInit", 1)) return(1);

    /* Call CVodeSetUserData to attach the user data structure */
    retval = CVodeSetUserData(cvode_mem, &rate_data);
    if (check_retval(&retval, "CVodeSetUserData", 1)) return(1);

    /* Call CVodeSVtolerances to specify the scalar relative tolerance
     * and vector absolute tolerances */
    retval = CVodeSVtolerances(cvode_mem, 1.0e-5, abstol);
    if (check_retval(&retval, "CVodeSVtolerances", 1)) return(1);

#ifndef USESPARSE
    /* Create sparse SUNMatrix for use in linear solves */
    A = SUNMatrix_cuSparse_NewBlockCSR(BATCHSIZE, nchem, nchem, nchem*nchem, cusp_handle);
    if(check_retval((void *)A, "SUNMatrix_cuSparse_NewBlockCSR", 0)) return(1);

    /* Set the sparsity pattern to be fixed so that the row pointers
     * and column indicies are not zeroed out by SUNMatZero */
    retval = SUNMatrix_cuSparse_SetFixedPattern(A, 1);

    /* Initialiize the Jacobian with its fixed sparsity pattern */
    blockJacInit(A);
#else
    int nnz = {{network.get_sparse_matrix_component(return_type="nsparse")}};
    /* Create sparse SUNMatrix for use in linear solves */
    A = SUNMatrix_cuSparse_NewBlockCSR(BATCHSIZE, nchem, nchem, nnz, cusp_handle);
    if(check_retval((void *)A, "SUNMatrix_cuSparse_NewBlockCSR", 0)) return(1);
    /* Set the sparsity pattern to be fixed so that the row pointers
     * and column indicies are not zeroed out by SUNMatZero */
    retval = SUNMatrix_cuSparse_SetFixedPattern(A, 1);
    blockSparseJacInit(A);
#endif

    /* Create the SUNLinearSolver object for use by CVode */
    LS = SUNLinSol_cuSolverSp_batchQR(y, A, cusol_handle);
    if(check_retval((void *)LS, "SUNLinSol_cuSolverSp_batchQR", 0)) return(1);

    /* Call CVodeSetLinearSolver to attach the matrix and linear solver to CVode */
    retval = CVodeSetLinearSolver(cvode_mem, LS, A);
    if(check_retval(&retval, "CVodeSetLinearSolver", 1)) return(1);

    /* Set the user-supplied Jacobian routine Jac */

#ifndef USESPARSE
    retval = CVodeSetJacFn(cvode_mem, calculate_jacobian_{{solver_name}});
    if(check_retval(&retval, "CVodeSetJacFn", 1)) return(1);
#else
    retval = CVodeSetJacFn(cvode_mem, calculate_sparse_jacobian_{{solver_name}});
    if(check_retval(&retval, "CVodeSetJacFn", 1)) return(1);
#endif

    return 0;
}


int DengoSolver::EvolveChemistry(double dtf, double z, double *input, double *rtol, double *atol, unsigned long dims)
{
    unsigned long ntimes = dims/BATCHSIZE;
    unsigned long eqlast = dims % BATCHSIZE;
    unsigned long count  = 0;
    int retval;
    realtype t;
    realtype reltol = rtol[0];

    // split input by batchsize
    for (count = 0; count < ntimes; count++)
    {
        for (int i = 0; i < neq; i++)
        {
            ydata[i]       = input[count*neq+i];
            abstol_data[i] = reltol*reltol*input[count*neq+i];
        }
        //for (int i = 0; i < nchem; i++) printf("ydata[%d] = %0.5g\n", i, ydata[i]);

        N_VCopyToDevice_Cuda(y);
        N_VCopyToDevice_Cuda(abstol);

        retval = CVodeReInit(cvode_mem, 0.0, y);
        retval = CVode(cvode_mem, dtf, y, &t, CV_NORMAL);
        N_VCopyFromDevice_Cuda(y);

        // copy cvode output from cuda vector
        for (int i = 0; i < neq; i++) input[count*neq+i] = ydata[i];

        //for (int i = 0; i < nchem; i++) printf("outydata[%d] = %0.5g\n", i, ydata[i]);
    }
    return 0;

}


// Initialize a data object that stores the reaction/ cooling rate data
{{solver_name}}_data {{solver_name}}_setup_data(int *NumberOfFields, char ***FieldNames)
{

    //-----------------------------------------------------
    // Function : {{solver_name}}_setup_data
    // Description: Initialize a data object that stores the reaction/ cooling rate data
    //-----------------------------------------------------


    // let's not re-scale the data yet...
    {{solver_name}}_data ratedata;

    ratedata.nbins = {{network.T |length}};
    ratedata.dbin = (log( {{network.T_bounds[1]}})-log({{network.T_bounds[0]}})) / {{network.T |length - 1}};
    ratedata.idbin = 1.0 / ratedata.dbin;
    ratedata.lb   = log({{network.T_bounds[0]}});
    ratedata.ub   = log({{network.T_bounds[1]}});

    /* Redshift-related pieces */
    /*
       data->z_bounds[0] = {{ network.z_bounds[0] }};
       data->z_bounds[1] = {{ network.z_bounds[1] }};
       data->n_zbins = {{ network.z | length }} - 1;
       data->d_zbin = (log(data->z_bounds[1] + 1.0) - log(data->z_bounds[0] + 1.0)) / data->n_zbins;
       data->id_zbin = 1.0L / data->d_zbin;
     */


    // initialize memory space for reaction rates and cooling rates
    // we use managed data, so the pointer can simultaneously be accessed from device and the host
    {%- for name, rate in network.reactions | dictsort %}
    {% if 'pi' not in name -%}
    cudaMallocManaged(&ratedata.r_{{name}}, sizeof(double)*{{network.T | length }});
    cudaMallocManaged(&ratedata.rs_{{name}}, sizeof(double)*{{network.T | length }});
    cudaMallocManaged(&ratedata.drs_{{name}}, sizeof(double)*{{network.T | length }});
    {%- else -%}
    cudaMallocManaged(&ratedata.r_{{name}}, sizeof(double)*{{network.z | length }});
    cudaMallocManaged(&ratedata.rs_{{name}}, sizeof(double)*{{network.z | length }});
    cudaMallocManaged(&ratedata.drs_{{name}}, sizeof(double)*{{network.z | length }});
    {%- endif %}
    {%-endfor %}

    // Cooling Rates
    {%- for name, rate in network.cooling_actions | dictsort %}
    {%- for name2 in rate.tables | sort %}
    {% if 'ph' not in name -%}
    cudaMallocManaged(&ratedata.c_{{name}}_{{name2}}, sizeof(double)* {{network.T | length}} );
    cudaMallocManaged(&ratedata.cs_{{name}}_{{name2}}, sizeof(double)* {{network.T | length}} );
    cudaMallocManaged(&ratedata.dcs_{{name}}_{{name2}}, sizeof(double)* {{network.T | length}} );
    {%- else -%}
    cudaMallocManaged(&ratedata.c_{{name}}_{{name2}}, sizeof(double)* {{network.T | length}} );
    cudaMallocManaged(&ratedata.cs_{{name}}_{{name2}}, sizeof(double)* {{network.T | length}} );
    cudaMallocManaged(&ratedata.dcs_{{name}}_{{name2}}, sizeof(double)* {{network.T | length}} );
    {%- endif %}
    {%- endfor %}
    {%- endfor %}

    // initialize memory space for the temperature-related pieces
    cudaMallocManaged(&ratedata.Ts, sizeof(double)* BATCHSIZE);
    cudaMallocManaged(&ratedata.logTs, sizeof(double)* BATCHSIZE);
    cudaMallocManaged(&ratedata.Tdef,  sizeof(double)* BATCHSIZE);
    cudaMallocManaged(&ratedata.dTs_ge,  sizeof(double)* BATCHSIZE);
    cudaMallocManaged(&ratedata.Tge,  sizeof(double)* BATCHSIZE);

    // gamma as a function of temperature
    /*
       {%- for sp in network.interpolate_gamma_species_name | sort %}
       cudaMallocManaged(&ratedata.g_gamma{{sp}}, sizeof(double)* BATCHSIZE);
       cudaMallocManaged(&ratedata.g_dgamma{{sp}}_dT,  sizeof(double)* BATCHSIZE);
       cudaMallocManaged(&ratedata.gamma{{sp}},  sizeof(double)* BATCHSIZE);
       cudaMallocManaged(&ratedata.dgamma_dT{{sp}},  sizeof(double)* BATCHSIZE);
       cudaMallocManaged(&ratedata._gamma{{sp}}_dT, sizeof(double)*BATCHSIZE);
       {%- endfor %}

    // maybe we can calculate the density on the fly
    // space to store the mass density
    cudaMallocManaged(&ratedata.mdensity, sizeof(double)* BATCHSIZE);
    cudaMallocManaged(&ratedata.inv_mdensity, sizeof(double)* BATCHSIZE);
     */
    // extra stuff like the density-dependent cooling rate

    {%- if "cie_cooling" in network.cooling_actions %}
    cudaMallocManaged(&ratedata.cie_optical_depth_approx, sizeof(double)* BATCHSIZE);
    {%- endif %}
    {%- if "gloverabel08" in network.cooling_actions %}
    cudaMallocManaged(&ratedata.h2_optical_depth_approx, sizeof(double)* BATCHSIZE);
    {%- endif %}

    return ratedata;
}

{% block read_tables %}
void {{ solver_name }}_read_rate_tables({{solver_name}}_data *data)
{
    hid_t file_id = H5Fopen("{{ solver_name }}_tables.h5", H5F_ACC_RDONLY, H5P_DEFAULT);
    /* Allocate the correct number of rate tables */

    {%- for name, rate in network.reactions | dictsort %}
    H5LTread_dataset_double(file_id, "/{{ name }}", data->r_{{name}});
    {%- endfor %}

    H5Fclose(file_id);
}


void {{ solver_name }}_read_cooling_tables({{solver_name}}_data *data)
{

    hid_t file_id = H5Fopen("{{ solver_name }}_tables.h5", H5F_ACC_RDONLY, H5P_DEFAULT);
    /* Allocate the correct number of rate tables */

    {%- for name, rate in network.cooling_actions | dictsort %}
    {%- for name2 in rate.tables | sort %}
    H5LTread_dataset_double(file_id, "/{{name}}_{{name2}}",
            data->c_{{name}}_{{name2}});
    {%- endfor %}
    {%- endfor %}

    H5Fclose(file_id);
}

/*
   void {{ solver_name }}_read_gamma({{solver_name}}_data *data)
   {

   hid_t file_id = H5Fopen("{{ solver_name }}_tables.h5", H5F_ACC_RDONLY, H5P_DEFAULT);
// Allocate the correct number of rate tables


{%- for sp in network.interpolate_gamma_species_name | sort %}
H5LTread_dataset_double(file_id, "/gamma{{sp}}",
data->g_gamma{{sp}} );
H5LTread_dataset_double(file_id, "/dgamma{{sp}}_dT",
data->g_dgamma{{sp}}_dT );
{% endfor %}

H5Fclose(file_id);

}
{% endblock %} {# read_tables #}
 */


// interpolation kernel
// ideally, we should use texture to do interpolation,
// let's ignore it for now, cos i guess most time is spent in doing the matrix thingy

    __global__
void linear_interpolation_kernel({{solver_name}}_data data)
{
    int j = threadIdx.x + blockDim.x* blockIdx.x;

    int k;
    double Tdef, t1;

    {%- for name, rate in network.reactions | dictsort %}
    double *{{name}} = data.r_{{name}};
    double *rs_{{name}}  = data.rs_{{name}};
    double *drs_{{name}} = data.drs_{{name}};
    {%- endfor %}

    {%- for name, rate in network.cooling_actions | dictsort %}
    {%- for name2 in rate.tables | sort %}
    double *{{name}}_{{name2}} = data.c_{{name}}_{{name2}};
    double *cs_{{name}}_{{name2}}  = data.cs_{{name}}_{{name2}};
    double *dcs_{{name}}_{{name2}} = data.dcs_{{name}}_{{name2}};
    {%- endfor %}
    {%- endfor %}

    if (j < BATCHSIZE)
    {
        k    = __float2int_rz(data.idbin*data.logTs[j] - data.lb);
        t1   = data.lb + k*data.dbin;
        Tdef = (data.logTs[j] - t1) * data.idbin;

        {%- for name, rate in network.reactions       | dictsort %}
        rs_{{name}}[j] = Tdef*{{name}}[k+1] + (-{{name}}[k]*Tdef + {{name}}[k]);
        {%- endfor %}


        {%- for name, rate in network.cooling_actions | dictsort %}
        {%- for name2 in rate.tables | sort %}
        cs_{{name}}_{{name2}}[j] = Tdef*{{name}}_{{name2}}[k+1] + (-{{name}}_{{name2}}[k]*Tdef + {{name}}_{{name2}}[k]);
        {%- endfor %}
        {%- endfor %}

    }
}


    __global__
static void rhs_kernel(double y, double *ydata, double *ydotdata, {{solver_name}}_data data)
{
    int i = blockIdx.x* blockDim.x + threadIdx.x;

    int groupi = i * nchem;

    // get rate pointer
    {%- for name, rate in network.reactions | dictsort %}
    double *{{name}} = data.rs_{{name}};
    {%-endfor%}

    {%- for name, rate in network.cooling_actions | dictsort %}
    {%- for name2 in rate.tables | sort %}
    double *{{name}}_{{name2}} = data.cs_{{name}}_{{name2}};
    {%-endfor%}
    {%-endfor%}


    {% if "gloverabel08" in network.cooling_actions %}
    double h2_optical_depth_approx;
    {% endif %}
    {% if "cie_cooling" in network.cooling_actions %}
    double cie_optical_depth_approx;
    {% endif %}

    int j;
    double z, T, mdensity, inv_mdensity;

    if (i < BATCHSIZE)
    {
        T = data.Ts[i];
        z = data.current_z;

        {% if "gloverabel08" in network.cooling_actions %}
        h2_optical_depth_approx = 1.0; // data.h2_optical_depth_approx[i];
        {% endif %}
        {% if "cie_cooling" in network.cooling_actions %}
        cie_optical_depth_approx = 1.0; // data.cie_optical_depth_approx[i];
        {% endif %}

        {%- for sp in network.required_species | sort %}
        double {{sp.name}} = ydata[groupi+{{loop.index0}}];
        {%- endfor %}

        mdensity     = mh*({{network.print_mass_density()}});
        inv_mdensity = 1.0/mdensity;

        {%- for species in network.required_species | sort %}
        //
        // Species: {{species.name}}
        //
        j = {{loop.index0}};
        {{network.print_ccode(species, assign_to="ydotdata[groupi+j]" ) }}
        {% if species.name == "ge" %}
        ydotdata[groupi+j] *= inv_mdensity;
        {% endif %}
        j++;
        {%- endfor %}

    }

    /*
       for (int ii = 0; ii < {{network.required_species| length}}; ii++)
       {
       printf("from %d: ydot[%d] = %0.5g\n", i, ii, ydotdata[groupi+ii]);
       }
     */

}


    __global__
void temperature_kernel(double* ydata, {{solver_name}}_data data)
{
    int i = blockIdx.x* blockDim.x + threadIdx.x;
    int groupi = i * nchem;

    double *temperature = data.Ts;
    double *logTs      = data.logTs;
    double *Tge        = data.Tge;

    double gammaH2_1 = 7./5.;
    double gammaH2_2 = 7./5.;
    // as of now just do not use any "temperature-dependent" gamma
    // which simplifies my life, and not having the need to iterate it to convergence

    if (i < BATCHSIZE)
    {
        {%- for sp in network.required_species | sort %}
        double {{sp.name}} = ydata[groupi+{{loop.index0}}];
        {%- endfor %}
        double density = {{network.print_mass_density()}};
        temperature[i] = {{network.temperature_calculation()}};
        logTs      [i] = log(temperature[i]);
        Tge        [i] = 0.0; //TODO: update this to dT_dge;
    }
}

// Function Called by the solver
static int calculate_rhs_{{solver_name}}(realtype t, N_Vector y, N_Vector ydot, void *user_data)
{
    {{solver_name}}_data *udata = ({{solver_name}}_data *) user_data;
    double *ydata    = N_VGetDeviceArrayPointer_Cuda(y);
    double *ydotdata = N_VGetDeviceArrayPointer_Cuda(ydot);

    // calculate temperature kernel
    temperature_kernel<<<GRIDSIZE, BLOCKSIZE>>> (ydata, *udata);
    // interpolate the rates with updated temperature
    linear_interpolation_kernel<<<GRIDSIZE, BLOCKSIZE>>>(*udata);
    // update ydot with the kernel function
    rhs_kernel<<<GRIDSIZE, BLOCKSIZE>>>(t, ydata, ydotdata, *udata);

    cudaDeviceSynchronize();
    cudaError_t cuerr = cudaGetLastError();
    if (cuerr != cudaSuccess) {
        fprintf(stderr,
                ">>> ERROR in f: cudaGetLastError returned %s\n",
                cudaGetErrorName(cuerr));
        return(-1);
    }


    return 0;

}


// write jacobian



/*
 * taken from cvRoberts_block_cusolversp_batchqr.cu
 *
 * Jacobian initialization routine. This sets the sparisty pattern of
 * the blocks of the Jacobian J(t,y) = df/dy. This is performed on the CPU,
 * and only occurs at the beginning of the simulation.
 */
static int blockSparseJacInit(SUNMatrix J)
{
    int nnz = {{network.get_sparse_matrix_component(return_type="nsparse")}};

    int rowptrs[nchem+1];
    int colvals[nnz];

    SUNMatZero(J);

    {%- for rowptr in network.get_sparse_matrix_component(sparse_type="CSR", return_type="indexptrs") %}
    rowptrs[{{loop.index0}}] = {{rowptr}};
    {%- endfor %}
    rowptrs[nchem] = nnz;

    {%- for colvals, jac_comp, s1, s2, k in network.get_sparse_matrix_component(sparse_type = "CSR", return_type="component", assign_to = "")%}
        // {{s1.name}} by {{s2.name}}
        colvals[{{k}}] = {{colvals}};
    {%- endfor %}

    // copy rowptrs, colvals to the device
    SUNMatrix_cuSparse_CopyToDevice(J, NULL, rowptrs, colvals);
    cudaDeviceSynchronize();
    return (0);
}

static int blockJacInit(SUNMatrix J)
{
    int nnz = nchem*nchem;

    int rowptrs[nchem+1];
    int colvals[nnz];

    SUNMatZero(J);
    for (int r = 0; r < nchem+1; r++)
    {
        rowptrs[r] = r*nchem;
        // printf("rowptrs[%d] = %d\n", r, rowptrs[r]);
    }

    int bIdx;
    for (int c = 0; c < nnz; c++)
    {
        bIdx = c /nnz;
        colvals[c] = bIdx*nchem + c%nchem;
        // printf("colvals[%d] = %d\n", c, colvals[c]);
    }
    // copy rowptrs, colvals to the device
    SUNMatrix_cuSparse_CopyToDevice(J, NULL, rowptrs, colvals);
    cudaDeviceSynchronize();
    return (0);
}


static int JacInit(SUNMatrix J)
{
    int rowptrs[{{network.required_species|length}}+1];
    int colvals[{{network.required_species|length}} * {{network.required_species|length}}];

    /* Zero out the Jacobian */
    SUNMatZero(J);

    /* there are {{network.required_species|length}} entries per row*/
    {%- for sp in network.required_species %}
    rowptrs[{{loop.index0}}] = {{loop.index0}} * {{network.required_species | length}};
    {%- endfor %}

    {%- for sp  in network.required_species %}
    {% set outer_loop = loop %}
    // {{outer_loop.index0}} row of block
    {%- for sp2 in network.required_species %}
    colvals[{{ outer_loop.index0 *(network.required_species|length) + loop.index0}}] = {{loop.index0}};
    {%- endfor %}
    {%- endfor %}

    // copy rowptrs, colvals to the device
    SUNMatrix_cuSparse_CopyToDevice(J, NULL, rowptrs, colvals);
    cudaDeviceSynchronize();
    return (0);
}


/* Jacobian evaluation with GPU */
    __global__
static void jacobian_kernel(realtype *ydata, realtype *Jdata, {{solver_name}}_data data)
{
    int groupj;

    // temporary:
    int nnzper = nchem*nchem;
    int i;
    double *Tge = data.Tge;
    double z, T;

    {% if "gloverabel08" in network.cooling_actions %}
    double *h2_optical_depth_approx_arr = data.h2_optical_depth_approx;
    {% endif %}
    {% if "cie_cooling" in network.cooling_actions %}
    double *cie_optical_depth_approx_arr = data.cie_optical_depth_approx;
    {% endif %}
    groupj = blockIdx.x*blockDim.x + threadIdx.x;

    T = 1000.0;
    z = 0.0;

    if (groupj < BATCHSIZE)
    {
        i = groupj;
        {% if "gloverabel08" in network.cooling_actions %}
        double h2_optical_depth_approx = 1.0; //h2_optical_depth_approx_arr[i];
        {% endif %}
        {% if "cie_cooling" in network.cooling_actions %}
        double cie_optical_depth_approx = 1.0; //cie_optical_depth_approx_arr[i];
        {% endif %}
        // pulled the species data
        {%- for sp in network.required_species | sort %}
        double {{sp.name}} = ydata[nchem*groupj+{{loop.index0}}];
        {%- endfor %}
        double mdensity = mh * ({{network.print_mass_density()}});
        double inv_mdensity = 1.0/ mdensity;

        {%- for name, rate in network.reactions | dictsort %}
        double *{{name}} = data.rs_{{name}};
        double *r{{name}}= data.drs_{{name}};
        {%- endfor %}

        {%- for name, rate in network.cooling_actions | dictsort %}
        {%- for name2 in rate.tables | sort %}
        double *{{name}}_{{name2}} = data.cs_{{name}}_{{name2}};
        double *r{{name}}_{{name2}} = data.dcs_{{name}}_{{name2}};
        {%- endfor %}
        {%- endfor %}

        {%- for s2 in network.required_species | sort %}
        //
        // Species: {{s2.name}}
        //
        {% set i_s2 = loop %}
        {%- for s1 in network.required_species | sort %}
        {% set i_s1 = loop%}
        // {{s2.name}} by {{s1.name}}

        {% if  network.print_jacobian_component(s2, s1, print_zeros = False) != None %}
        {{ network.print_jacobian_component(s2, s1, assign_to="Jdata[nnzper*groupj + {0}*nchem+ {1}]".format( i_s2.index0, i_s1.index0)) }}
        {% else %}
        // because the Jacobian is initialized to zeros by default
        Jdata[nnzper*groupj+ {{i_s2.index0}}*nchem+ {{i_s1.index0}}] = ZERO;
        {% endif %}

        {% if s2.name == 'ge' %}
        Jdata[nnzper*groupj+ {{i_s2.index0}}*nchem+ {{i_s1.index0}}] *= inv_mdensity;
        {% endif %}

        {% if s1.name == 'ge' %}
        Jdata[nnzper*groupj+ {{i_s2.index0}}*nchem+ {{i_s1.index0}}] *= T{{network.energy_term.name}}[i];
        {% endif %}

        {%- endfor %}
        {%- endfor %}

    }
    /*
       if (groupj < 1){
       for (int i =0; i < 100; i++){
       printf("from %d: Jdata[%d] = %0.5g\n", groupj, i, Jdata[nnzper*groupj+i]);
       }
       printf("\n");
       }
     */
}

    __global__
static void sparse_jacobian_kernel(realtype *ydata, realtype *Jdata, {{solver_name}}_data data)
{
    int groupj;

    // temporary:
    int nnzper = {{network.get_sparse_matrix_component(return_type="nsparse")}};
    int i;
    double *Tge = data.Tge;
    double z, T;

    {%- if "gloverabel08" in network.cooling_actions %}
    double *h2_optical_depth_approx_arr = data.h2_optical_depth_approx;
    {%- endif %}
    {%- if "cie_cooling" in network.cooling_actions %}
    double *cie_optical_depth_approx_arr = data.cie_optical_depth_approx;
    {%- endif %}
    groupj = blockIdx.x*blockDim.x + threadIdx.x;

    T = 1000.0;
    z = 0.0;

    if (groupj < BATCHSIZE)
    {
        i = groupj;
        {%- if "gloverabel08" in network.cooling_actions %}
        double h2_optical_depth_approx = 1.0; //h2_optical_depth_approx_arr[i];
        {%- endif %}
        {%- if "cie_cooling" in network.cooling_actions %}
        double cie_optical_depth_approx = 1.0; //cie_optical_depth_approx_arr[i];
        {%- endif %}
        // pulled the species data
        {%- for sp in network.required_species | sort %}
        double {{sp.name}} = ydata[nchem*groupj+{{loop.index0}}];
        {%- endfor %}
        double mdensity = mh * ({{network.print_mass_density()}});
        double inv_mdensity = 1.0/ mdensity;

        {%- for name, rate in network.reactions | dictsort %}
        double *{{name}} = data.rs_{{name}};
        double *r{{name}}= data.drs_{{name}};
        {%- endfor %}

        {%- for name, rate in network.cooling_actions | dictsort %}
        {%- for name2 in rate.tables | sort %}
        double *{{name}}_{{name2}} = data.cs_{{name}}_{{name2}};
        double *r{{name}}_{{name2}} = data.dcs_{{name}}_{{name2}};
        {%- endfor %}
        {%- endfor %}

        {%- for colvals, jac_comp, s1, s2, k in network.get_sparse_matrix_component(sparse_type = "CSR", return_type="component", assign_to = "")%}
        // {{s1.name}} by {{s2.name}}
        Jdata[nnzper*groupj+{{k}}] {{jac_comp}}

        {% if s1.name == 'ge' %}
        Jdata[nnzper*groupj+{{k}}] *= inv_mdensity;
        {%- endif %}
        {%- if s2.name == 'ge' %}
        Jdata[nnzper*groupj+{{k}}] *= T{{ network.energy_term.name }}[i];
        {%- endif %}
        {%- endfor %}
    }
    /*
       if (groupj < 1){
       for (int i =0; i < 100; i++){
       printf("from %d: Jdata[%d] = %0.5g\n", groupj, i, Jdata[nnzper*groupj+i]);
       }
       printf("\n");
       }
     */
}

/*
 * Jacobian routine. COmpute J(t,y) = df/dy.
 * This is done on the GPU.
 */
static int calculate_jacobian_{{solver_name}}(realtype t, N_Vector y, N_Vector fy, SUNMatrix J,
        void *user_data, N_Vector tmp1, N_Vector tmp2, N_Vector tmp3)
{
    {{solver_name}}_data *data = ({{solver_name}}_data*)user_data;

    int nnzper;
    realtype *Jdata, *ydata;
    nnzper = {{network.required_species|length}}* {{network.required_species|length}};
    ydata = N_VGetDeviceArrayPointer_Cuda(y);
    Jdata = SUNMatrix_cuSparse_Data(J);

    jacobian_kernel<<<GRIDSIZE, BLOCKSIZE>>>(ydata, Jdata, *data);

    cudaDeviceSynchronize();
    cudaError_t cuerr = cudaGetLastError();
    if (cuerr != cudaSuccess) {
        fprintf(stderr, ">>> ERROR in Jac: cudaGetLastError returned %s\n",
                cudaGetErrorName(cuerr));
        return(-1);
    }

    return(0);

}

static int calculate_sparse_jacobian_{{solver_name}}(realtype t, N_Vector y, N_Vector fy, SUNMatrix J,
        void *user_data, N_Vector tmp1, N_Vector tmp2, N_Vector tmp3)
{
    {{solver_name}}_data *data = ({{solver_name}}_data*)user_data;

    realtype *Jdata, *ydata;
    ydata = N_VGetDeviceArrayPointer_Cuda(y);
    Jdata = SUNMatrix_cuSparse_Data(J);

    sparse_jacobian_kernel<<<GRIDSIZE, BLOCKSIZE>>>(ydata, Jdata, *data);

    cudaDeviceSynchronize();
    cudaError_t cuerr = cudaGetLastError();
    if (cuerr != cudaSuccess) {
        fprintf(stderr, ">>> ERROR in Jac: cudaGetLastError returned %s\n",
                cudaGetErrorName(cuerr));
        return(-1);
    }

    return(0);

}

// now write tests kit

void test_interpolation_kernel({{solver_name}}_data data)
{
    // initialize temperature;
    for (int i = 0; i < BATCHSIZE; i++)
    {
        data.Ts[i] = (double) 3000.0 * (i+10)/BATCHSIZE;
        data.logTs[i] = log(data.Ts[i]);
    }

    float time;
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    cudaEventRecord(start, 0);
    for (int j = 0; j < 8192; j++){
        linear_interpolation_kernel<<<GRIDSIZE, BLOCKSIZE>>>(data);
    }
    cudaEventRecord(stop, 0);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&time, start, stop);
    printf("Time to generate:  %3.1f ms \n", time);

    cudaDeviceSynchronize();
    cudaError_t cuerr = cudaGetLastError();
    if (cuerr != cudaSuccess) {
        fprintf(stderr,
                ">>> ERROR in interpolation_kernel: cudaGetLastError returned %s\n",
                cudaGetErrorName(cuerr));
    }
}

void initialize_ydata(double *ydata, int NSYSTEM)
{
    double h2fraction = 1.0e-5;
    double efraction  = 1.0e-5;
    double density    = 1.0e0;
    double temperature = 1000.0;
    double tiny_fraction = 1.0e-20;
    for (int i = 0; i < NSYSTEM; i++)
    {
        // H2I
        ydata[i*nchem]   = density*h2fraction*0.76 /2.;
        // H2II
        ydata[i*nchem+1] = density*tiny_fraction;
        // HI
        ydata[i*nchem+2] = density*0.76*(1-h2fraction);
        // HII
        ydata[i*nchem+3] = density*efraction;
        // H-
        ydata[i*nchem+4] = density*tiny_fraction;
        // HeI
        ydata[i*nchem+5] = density*0.24 / 4.;
        // HeII
        ydata[i*nchem+6] = density*tiny_fraction;
        // HeIII
        ydata[i*nchem+7] = density*tiny_fraction;
        // de
        ydata[i*nchem+8] = density*efraction;
        // ge
        ydata[i*nchem+9] = 3./2.*kb* temperature / mh / density ;

    }
}


void test_temperature_kernel({{solver_name}}_data data)
{
    int neq = BATCHSIZE*nchem;

    N_Vector y = N_VNew_Cuda(neq);
    double *ydata;
    ydata = N_VGetHostArrayPointer_Cuda(y);
    initialize_ydata(ydata, BATCHSIZE);
    N_VCopyToDevice_Cuda(y);


    ydata = N_VGetDeviceArrayPointer_Cuda(y);
    temperature_kernel<<<GRIDSIZE,BLOCKSIZE>>>(ydata, data);

    for (int i = 0; i<BATCHSIZE; i++){
        printf("temperature[%d] = %0.5g\n", i, data.Ts[i]);
    }

    cudaDeviceSynchronize();
    cudaError_t cuerr = cudaGetLastError();
    if (cuerr != cudaSuccess) {
        fprintf(stderr,
                ">>> ERROR in temperature kernel: cudaGetLastError returned %s\n",
                cudaGetErrorName(cuerr));
    }
}



void test_rhs_function({{solver_name}}_data data)
{
    double t = 1.0;
    int neq = BATCHSIZE*nchem;

    N_Vector y = N_VNew_Cuda(neq);
    double *ydata;
    ydata = N_VGetHostArrayPointer_Cuda(y);
    initialize_ydata(ydata, BATCHSIZE);
    N_VCopyToDevice_Cuda(y);


    ydata = N_VGetDeviceArrayPointer_Cuda(y);
    N_Vector ydot = N_VNew_Cuda(neq);

    calculate_rhs_{{solver_name}}(t, y, ydot, &data);
    //f(realtype t, N_Vector y, N_Vector ydot, void *user_data)
}


void test_jacobian_function({{solver_name}}_data data)
{
    double t = 1.0;
    int neq = BATCHSIZE*nchem;

    N_Vector y = N_VNew_Cuda(neq);
    double *ydata;
    ydata = N_VGetHostArrayPointer_Cuda(y);
    initialize_ydata(ydata, BATCHSIZE);
    N_VCopyToDevice_Cuda(y);

    ydata = N_VGetDeviceArrayPointer_Cuda(y);
    N_Vector ydot = N_VNew_Cuda(neq);

    // also need to initialize jacobian data space

    /* Create sparse SUNMatrix for use in linear solves */
    SUNMatrix A;
    A = NULL;
    cusparseHandle_t cusp_handle;
    cusparseCreate(&cusp_handle);
    A = SUNMatrix_cuSparse_NewBlockCSR(BATCHSIZE, nchem, nchem, nchem*nchem, cusp_handle);
    /* Initialiize the Jacobian with its fixed sparsity pattern */
    blockJacInit(A);

    calculate_jacobian_{{solver_name}}(t, y, y, A, &data, y, y, y);
    //f(realtype t, N_Vector y, N_Vector ydot, void *user_data)
}

/*
 * Private Helper Function
 * Get and print some final statistics
 */

/*
 * Check function return value...
 *   opt == 0 means SUNDIALS function allocates memory so check if
 *            returned NULL pointer
 *   opt == 1 means SUNDIALS function returns an integer value so check if
 *            retval < 0
 *   opt == 2 means function allocates memory so check if returned
 *            NULL pointer
 */

static int check_retval(void *returnvalue, const char *funcname, int opt)
{
    int *retval;

    /* Check if SUNDIALS function returned NULL pointer - no memory allocated */
    if (opt == 0 && returnvalue == NULL) {
        fprintf(stderr, "\nSUNDIALS_ERROR: %s() failed - returned NULL pointer\n\n",
                funcname);
        return(1); }

    /* Check if retval < 0 */
    else if (opt == 1) {
        retval = (int *) returnvalue;
        if (*retval < 0) {
            fprintf(stderr, "\nSUNDIALS_ERROR: %s() failed with retval = %d\n\n",
                    funcname, *retval);
            return(1); }}

    /* Check if function returned NULL pointer - no memory allocated */
    else if (opt == 2 && returnvalue == NULL) {
        fprintf(stderr, "\nMEMORY_ERROR: %s() failed - returned NULL pointer\n\n",
                funcname);
        return(1); }

    return(0);
}

static void PrintFinalStats(void *cvode_mem, SUNLinearSolver LS)
{
    long int nst, nfe, nsetups, nje, nni, ncfn, netf, nge;
    size_t cuSpInternalSize, cuSpWorkSize;
    int retval;

    retval = CVodeGetNumSteps(cvode_mem, &nst);
    check_retval(&retval, "CVodeGetNumSteps", 1);
    retval = CVodeGetNumRhsEvals(cvode_mem, &nfe);
    check_retval(&retval, "CVodeGetNumRhsEvals", 1);
    retval = CVodeGetNumLinSolvSetups(cvode_mem, &nsetups);
    check_retval(&retval, "CVodeGetNumLinSolvSetups", 1);
    retval = CVodeGetNumErrTestFails(cvode_mem, &netf);
    check_retval(&retval, "CVodeGetNumErrTestFails", 1);
    retval = CVodeGetNumNonlinSolvIters(cvode_mem, &nni);
    check_retval(&retval, "CVodeGetNumNonlinSolvIters", 1);
    retval = CVodeGetNumNonlinSolvConvFails(cvode_mem, &ncfn);
    check_retval(&retval, "CVodeGetNumNonlinSolvConvFails", 1);

    retval = CVodeGetNumJacEvals(cvode_mem, &nje);
    check_retval(&retval, "CVodeGetNumJacEvals", 1);

    retval = CVodeGetNumGEvals(cvode_mem, &nge);
    check_retval(&retval, "CVodeGetNumGEvals", 1);

    SUNLinSol_cuSolverSp_batchQR_GetDeviceSpace(LS, &cuSpInternalSize, &cuSpWorkSize);

    printf("\nFinal Statistics:\n");
    printf("nst = %-6ld nfe  = %-6ld nsetups = %-6ld nje = %ld\n",
            nst, nfe, nsetups, nje);
    printf("nni = %-6ld ncfn = %-6ld netf = %-6ld    nge = %ld\n \n",
            nni, ncfn, netf, nge);
    printf("cuSolverSp numerical factorization workspace size (in bytes) = %ld\n", cuSpWorkSize);
    printf("cuSolverSp internal Q, R buffer size (in bytes) = %ld\n", cuSpInternalSize);
}


int run_solver(int argc, char *argv[])
{
    realtype reltol, t, tout;
    realtype *ydata, *abstol_data;
    N_Vector y, abstol;
    SUNMatrix A;
    SUNLinearSolver LS;
    void *cvode_mem;
    int retval, iout;
    int neq, ngroups, groupj;
    {{solver_name}}_data data = {{solver_name}}_setup_data(NULL, NULL);
    {{solver_name}}_read_cooling_tables( &data);
    {{solver_name}}_read_rate_tables( &data);

    cusparseHandle_t cusp_handle;
    cusolverSpHandle_t cusol_handle;

    y = abstol = NULL;
    A = NULL;
    LS = NULL;
    cvode_mem = NULL;

    /* Parse command line arguments */
    ngroups = BATCHSIZE;
    neq = ngroups* nchem;

    reltol = 1.0e-6;
    /* Initialize cuSOLVER and cuSPARSE handles */
    cusparseCreate(&cusp_handle);
    cusolverSpCreate(&cusol_handle);

    /* Create CUDA vector of length neq for I.C. and abstol */
    y = N_VNew_Cuda(neq);
    if (check_retval((void *)y, "N_VNew_Cuda", 0)) return(1);
    abstol = N_VNew_Cuda(neq);
    if (check_retval((void *)abstol, "N_VNew_Cuda", 0)) return(1);

    ydata = N_VGetHostArrayPointer_Cuda(y);
    abstol_data = N_VGetHostArrayPointer_Cuda(abstol);

    /* Initialize */
    initialize_ydata(ydata, BATCHSIZE);
    for (int i = 0; i < neq; i++){
        abstol_data[i] = 1.0e-25;
    }
    N_VCopyToDevice_Cuda(y);
    N_VCopyToDevice_Cuda(abstol);

    /* Call CVodeCreate to create the solver memory and specify the
     * Backward Differentiation Formula */
    cvode_mem = CVodeCreate(CV_BDF);
    if (check_retval((void *)cvode_mem, "CVodeCreate", 0)) return(1);

    /* Call CVodeInit to initialize the integrator memory and specify the
     * user's right hand side function in y'=f(t,y), the inital time T0, and
     * the initial dependent variable vector y. */
    retval = CVodeInit(cvode_mem, calculate_rhs_{{solver_name}}, T0, y);
    if (check_retval(&retval, "CVodeInit", 1)) return(1);

    /* Call CVodeSetUserData to attach the user data structure */
    retval = CVodeSetUserData(cvode_mem, &data);
    if (check_retval(&retval, "CVodeSetUserData", 1)) return(1);

    /* Call CVodeSVtolerances to specify the scalar relative tolerance
     * and vector absolute tolerances */
    retval = CVodeSVtolerances(cvode_mem, reltol, abstol);
    if (check_retval(&retval, "CVodeSVtolerances", 1)) return(1);

    /* Create sparse SUNMatrix for use in linear solves */
    A = SUNMatrix_cuSparse_NewBlockCSR(ngroups, nchem, nchem, nchem*nchem, cusp_handle);
    if(check_retval((void *)A, "SUNMatrix_cuSparse_NewBlockCSR", 0)) return(1);

    /* Set the sparsity pattern to be fixed so that the row pointers
     * and column indicies are not zeroed out by SUNMatZero */
    retval = SUNMatrix_cuSparse_SetFixedPattern(A, 1);

    /* Initialiize the Jacobian with its fixed sparsity pattern */
    blockJacInit(A);

    /* Create the SUNLinearSolver object for use by CVode */
    LS = SUNLinSol_cuSolverSp_batchQR(y, A, cusol_handle);
    if(check_retval((void *)LS, "SUNLinSol_cuSolverSp_batchQR", 0)) return(1);

    /* Call CVodeSetLinearSolver to attach the matrix and linear solver to CVode */
    retval = CVodeSetLinearSolver(cvode_mem, LS, A);
    if(check_retval(&retval, "CVodeSetLinearSolver", 1)) return(1);

    /* Set the user-supplied Jacobian routine Jac */
    retval = CVodeSetJacFn(cvode_mem, calculate_jacobian_{{solver_name}});
    if(check_retval(&retval, "CVodeSetJacFn", 1)) return(1);

    /* In loop, call CVode, print results, and test for error.
       Break out of loop when NOUT preset output times have been reached.  */
    printf(" \nGroup of independent 3-species kinetics problems\n\n");
    printf("number of groups = %d\n\n", ngroups);


    iout = 0;  tout = 1.0e13;

    retval = CVode(cvode_mem, tout, y, &t, CV_NORMAL);
    N_VCopyFromDevice_Cuda(y);
    for (groupj = 0; groupj < ngroups; groupj += 256) {
        printf("group %d: ", groupj);
        for (int i = 0; i < nchem; i++){
            printf("ydata[%d] = %0.5g\n", nchem*groupj+i, ydata[nchem*groupj+i]);
        }
        printf("\n");
    }

    /*
       if (check_retval(&retval, "CVode", 1)) break;
       if (retval == CV_SUCCESS) {
       iout++;
       tout *= TMULT;
       }

       if (iout == NOUT) break;
     */

    /* Print some final statistics */
    PrintFinalStats(cvode_mem, LS);

    /* Free y and abstol vectors */
    N_VDestroy(y);
    N_VDestroy(abstol);

    /* Free integrator memory */
    CVodeFree(&cvode_mem);

    /* Free the linear solver memory */
    SUNLinSolFree(LS);

    /* Free the matrix memory */
    SUNMatDestroy(A);

    /* Destroy the cuSOLVER and cuSPARSE handles */
    cusparseDestroy(cusp_handle);
    cusolverSpDestroy(cusol_handle);

    return(0);
}
void *setup_cvode_cuda_solver(CVRhsFn f, CVLsJacFn Jac, int NEQ, {{solver_name}}_data *data, SUNLinearSolver LS, SUNMatrix A, N_Vector y, double reltol, N_Vector abstol, cusparseHandle_t *cusp_handle, cusolverSpHandle_t *cusol_handle)
{
    int retval, iout;
    void *cvode_mem;
    cvode_mem = NULL;
    /* Call CVodeCreate to create the solver memory and specify the
     * Backward Differentiation Formula */
    cvode_mem = CVodeCreate(CV_BDF);
    //if (check_retval((void *)cvode_mem, "CVodeCreate", 0)) return(1);

    /* Call CVodeInit to initialize the integrator memory and specify the
     * user's right hand side function in y'=f(t,y), the inital time T0, and
     * the initial dependent variable vector y. */
    retval = CVodeInit(cvode_mem, f, 0.0, y);
    //if (check_retval(&retval, "CVodeInit", 1)) return(1);

    /* Call CVodeSetUserData to attach the user data structure */
    retval = CVodeSetUserData(cvode_mem, data);
    //if (check_retval(&retval, "CVodeSetUserData", 1)) return(1);

    /* Call CVodeSVtolerances to specify the scalar relative tolerance
     * and vector absolute tolerances */
    retval = CVodeSVtolerances(cvode_mem, reltol, abstol);
    //if (check_retval(&retval, "CVodeSVtolerances", 1)) return(1);

    /* Create sparse SUNMatrix for use in linear solves */
    A = SUNMatrix_cuSparse_NewBlockCSR(BATCHSIZE, nchem, nchem, nchem*nchem, *cusp_handle);
    //if(check_retval((void *)A, "SUNMatrix_cuSparse_NewBlockCSR", 0)) return(1);

    /* Set the sparsity pattern to be fixed so that the row pointers
     * and column indicies are not zeroed out by SUNMatZero */
    retval = SUNMatrix_cuSparse_SetFixedPattern(A, 1);

    /* Initialiize the Jacobian with its fixed sparsity pattern */
    blockJacInit(A);

    /* Create the SUNLinearSolver object for use by CVode */
    LS = SUNLinSol_cuSolverSp_batchQR(y, A, *cusol_handle);
    //if(check_retval((void *)LS, "SUNLinSol_cuSolverSp_batchQR", 0)) return(1);

    /* Call CVodeSetLinearSolver to attach the matrix and linear solver to CVode */
    retval = CVodeSetLinearSolver(cvode_mem, LS, A);
    //if(check_retval(&retval, "CVodeSetLinearSolver", 1)) return(1);

    /* Set the user-supplied Jacobian routine Jac */
    retval = CVodeSetJacFn(cvode_mem, calculate_jacobian_{{solver_name}});
    //if(check_retval(&retval, "CVodeSetJacFn", 1)) return(1);

    return cvode_mem;
}

int grid_performance()
{
    unsigned long dims = 16384;
    double density, temperature, h2fraction, efraction;
    double d, T;
    clock_t start, end;
    double cpu_time_used;

    cudaEvent_t startCuda, stopCuda;
    cudaEventCreate(&startCuda);
    cudaEventCreate(&stopCuda);
    float milliseconds = 0;

    density = 1.0;
    temperature = 10.0;
    h2fraction = 1e-4;
    efraction = 1e-4;

    const int nd = 9;
    const int nT = 9;
    double *timelapsed = new double[nd*nT];
    double *darray      = new double[nd*nT];
    double *Tarray      = new double[nd*nT];
    double *output      = new double[nchem*nd*nT];

    for (int i = 0; i < nd; i++)
    {
        for (int j = 0; j < nT; j++)
        {
            d = density* pow(10., i);
            T = temperature* pow(2.2, j);

            start = clock();
            cudaEventRecord(startCuda);
            run_dengo_struct(d, T, h2fraction, efraction, dims, &output[(i*nT+j)*nchem]);

            cudaEventRecord(stopCuda);
            end = clock();
            cpu_time_used = ((double) (end - start)) / CLOCKS_PER_SEC;

            printf("d=%0.5g; T=%0.5g;  %f seconds %lu cells; %0.5g percell \n", d, T, cpu_time_used, dims, cpu_time_used/ dims);
            cudaEventSynchronize(stopCuda);
            cudaEventElapsedTime(&milliseconds, startCuda, stopCuda);

            //printf("took %f milliseconds to execute %lu; %0.5g percell \n", milliseconds, dims, milliseconds/ dims);

            timelapsed[i*nT+j] = milliseconds*1e-3;
            darray    [i*nT+j] = d;
            Tarray    [i*nT+j] = T;


        }
    }

    // create a file
    hid_t file_id, dataset;
    hid_t datatype, dataspace;
    hsize_t dimsf[1], dimsO[1];
    herr_t status;
    dimsf[0] = nd*nT;
    dimsO[0] = nchem*nd*nT;

    file_id = H5Fcreate("performance.h5", H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    H5LTmake_dataset_double(file_id, "/time", 1, dimsf, timelapsed);
    H5LTmake_dataset_double(file_id, "/density", 1, dimsf, darray);
    H5LTmake_dataset_double(file_id, "/temperature", 1, dimsf, Tarray);
    H5LTmake_dataset_double(file_id, "/output", 1, dimsO, output);

    H5Fclose(file_id);

}

int test_scaling_dims()
{
    unsigned long dims = 4096;
    double density, temperature, h2fraction, efraction;
    clock_t start, end;
    double cpu_time_used;

    cudaEvent_t startCuda, stopCuda;
    cudaEventCreate(&startCuda);
    cudaEventCreate(&stopCuda);
    float milliseconds = 0;

    int ntimes = 8;
    double *timelapsed = new double[ntimes];
    long *ncells     = new long[ntimes];
    double *output   = new double[ntimes*nchem];

    density = 1.0;
    temperature = 1.0;
    h2fraction = 1e-4;
    efraction = 1e-4;

    for (int i = 0; i < ntimes; i++)
    {
        start = clock();
        cudaEventRecord(startCuda);

        run_dengo_struct(density, temperature, h2fraction, efraction, dims, &output[i*nchem]);

        dims *= 2;
        cudaEventRecord(stopCuda);
        end = clock();
        cpu_time_used = ((double) (end - start)) / CLOCKS_PER_SEC;

        printf("took %f seconds to execute %lu \n", cpu_time_used, dims);
        cudaEventSynchronize(stopCuda);

        cudaEventElapsedTime(&milliseconds, startCuda, stopCuda);

        printf("took %f milliseconds to execute %lu; %0.5g percell \n", milliseconds, dims, milliseconds/ dims);
        // measured the time lapsed
        // and the cells needed
        ncells[i] = dims;
        timelapsed[i] = milliseconds*1e-3;
    }

    // create a file
    hid_t file_id, dataset;
    hid_t datatype, dataspace;
    hsize_t dimsf[1], dimsO[1];
    herr_t status;
    dimsf[0] = ntimes;
    dimsO[0] = ntimes*nchem;

    file_id = H5Fcreate("scaling.h5", H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
    H5LTmake_dataset_double(file_id, "/time", 1, dimsf, timelapsed);
    H5LTmake_dataset_long(file_id, "/ncells", 1, dimsf, ncells);
    H5LTmake_dataset_double(file_id, "/output", 1, dimsO, output);
    H5Fclose(file_id);

}


int run_dengo_struct(double density, double temperature, double h2fraction, double efraction, unsigned long dims, double *output)
{
    // dengo solver interface
    DengoSolver s;

    double *input, *rtol, *atol;
    input = new double[dims*nchem];
    rtol  = new double[1];
    atol  = new double[dims*nchem];

    initialize_long_ydata(input, dims, density, temperature, h2fraction, efraction);
    rtol[0] = 1.0e-5;

    for (int i = 0; i<nchem; i++) printf("input[%d] = %0.5g\n", i, input[i]);

    // main evolution;
    double dtf = pow(6.67e-8*mh*density, -0.5);
    double z   = 0.0;

    s.EvolveChemistry(dtf, z, input, rtol, atol, dims);

    for (int i = 0; i<nchem; i++) printf("output[%d] = %0.5g\n", i, input[i]);

    // supposingly this would return the output in "input" array

/*
    double diff;
    for (unsigned long i = 0; i<dims; i++)
    {
        diff = (input[i] - input[i%nchem]) / input[i%nchem];
        if (fabs(diff) > rtol[0]){
            printf("output %lu diff[%d] = %0.5g; ref = %0.5g; out = %0.5g\n", i,  i%nchem, diff, input[i], input[i%nchem]);
        }
    }
*/

}



{% block main_evolution %}

int dengo_evolve_{{solver_name}} (double dtf, double &dt, double z, double *input,
        double *rtol, double *atol, unsigned long dims, {{solver_name}}_data *data, double *temp_array ){

    //-----------------------------------------------------
    // Function     : dengo_evolve_{{solver_name}}
    // Description  : Main ODE solver function in dengo

    // Parameter    :   dtf     : Desired time to be reached by the solver
    //                  dt      : Pointer to the actual time reached by the solver
    //                  z       : Current redshift
    //                  input   : Array to store the initial value of each species,
    //                            it will be updated with the value at time dt
    //                  rtol    : relative tolerance required for convergenece in each internal CVODE timesteps
    //                  atol    : absolute tolerance required for convergence in each interanl CVODE timesteps
    //                  dims    : dimension of the input array, i.e. no. of species * no. of cells
    //                  data    : {{solver_name}}_data object that relay the reaction/cooling rates, and normalizations
    //                  temp_array: temperature of each cell by the end of the evolution
    //
    //-----------------------------------------------------

    unsigned long i, j;
    int N = {{network.required_species | length}};

    {%- for species in network.required_species | sort %}
    double {{species.name}};
    {%- endfor %}

    {%- if not network.input_is_number %}
    for (i = 0; i < dims; i++) {
        j = i * N;
        {%- for species in network.ode_species | sort %}
        {%- if species.name != "ge" %}
        input[j] /= {{species.weight}}; // {{species.name}}
    {%- endif%}
    j++;
    {%- endfor%}
    }
    {%- endif %}

    CVRhsFn f    = calculate_rhs_{{solver_name}};
    CVLsJacFn jf = calculate_jacobian_{{solver_name}};

    if (dt < 0) dt = dtf / 1e0;
    data->current_z = z;
    int niter = 0;
    int siter = 0;

    double floor_value = 1e-20;

    // prepare memory space on device
    realtype *ydata, *abstol_data;
    N_Vector y, abstol;
    SUNMatrix A;
    SUNLinearSolver LS;
    void *cvode_mem;
    int retval, iout;
    int neq, ngroups, groupj;
    y = abstol = NULL;
    A = NULL;
    LS = NULL;
    cvode_mem = NULL;

    neq = BATCHSIZE*nchem;

    // Initialize cuSolver and suSPARSE
    cusparseHandle_t   cusp_handle;
    cusolverSpHandle_t cusol_handle;
    cusparseCreate  (&cusp_handle);
    cusolverSpCreate(&cusol_handle);

    /////////////////////////////////////////////////////////////////
    // Initialize memory spaces for cvode solver //
    /////////////////////////////////////////////////////////////////
    // Create CUDA vector of length neq for I.C. and abstol
    y = N_VNew_Cuda(neq);
    if (check_retval ((void *)y, "N_VNew_Cuda", 0)) return(1);
    abstol = N_VNew_Cuda(neq);
    if (check_retval ((void *)abstol, "N_VNew_Cuda", 0)) return(1);

    ydata = N_VGetHostArrayPointer_Cuda(y);
    abstol_data = N_VGetHostArrayPointer_Cuda(abstol);

    // Initialize cuda vector
    for (unsigned long i = 0; i < neq; i++){
        ydata[i] = input[i];
        abstol_data[i] = input[i]*1.0e-6;
    }
    N_VCopyToDevice_Cuda(y);
    N_VCopyToDevice_Cuda(abstol);

    /* Create the SUNLinearSolver object for use by CVode */
    // LS = SUNLinSol_cuSolverSp_batchQR(y, A, cusol_handle);
    // if(check_retval((void *)LS, "SUNLinSol_cuSolverSp_batchQR", 0)) return(1);
    cvode_mem = setup_cvode_cuda_solver(f, jf, neq, data, LS, A, y, rtol[0], abstol, &cusp_handle, &cusol_handle);

    //////////////////////////////////////////////////////////////////
    // Main Evolution /////////
    /////////////////////////////////////////////////////////////////

    unsigned long ntimes = dims/ BATCHSIZE;
    unsigned long eqlast = dims % BATCHSIZE;
    unsigned long count = 0;

    realtype t;
    realtype reltol = rtol[0];
    // split the input by batchsize,
    for (count = 0; count < ntimes; count++)
    {
        // update the yvector, and abstol
        for (int i = 0; i < neq; i++){
            ydata      [i] = input              [count*neq +i];
            abstol_data[i] = reltol*reltol*input[count*neq +i];
        }
        N_VCopyToDevice_Cuda(y);
        N_VCopyToDevice_Cuda(abstol);

        retval = CVodeReInit(cvode_mem, 0.0, y);
        retval = CVode(cvode_mem, dtf, y, &t, CV_NORMAL);
        N_VCopyFromDevice_Cuda(y);

        // copy the CVode output from cuda vector
        for (int i = 0; i < neq; i++){
            input[count*neq +i] = ydata[i];
        }
    }


    return 0;
}
{% endblock %}

void initialize_long_ydata(double *ydata, unsigned long NSYSTEM, double density, double temperature, double h2fraction, double efraction)
{
    double tiny_fraction = 1.0e-20;
    for (unsigned long i = 0; i < NSYSTEM; i++)
    {
        // H2I
        ydata[i*nchem]   = density*h2fraction*0.76 /2.;
        // H2II
        ydata[i*nchem+1] = density*tiny_fraction;
        // HI
        ydata[i*nchem+2] = density*0.76*(1-h2fraction);
        // HII
        ydata[i*nchem+3] = density*efraction;
        // H-
        ydata[i*nchem+4] = density*tiny_fraction;
        // HeI
        ydata[i*nchem+5] = density*0.24 / 4.;
        // HeII
        ydata[i*nchem+6] = density*tiny_fraction;
        // HeIII
        ydata[i*nchem+7] = density*tiny_fraction;
        // de
        ydata[i*nchem+8] = density*efraction;
        // ge
        ydata[i*nchem+9] = 3./2.*kb* temperature / mh;

    }
}

int run_dengo_solver(double density, double temperature, double h2fraction, double efraction, unsigned long dims)
{
    {{solver_name}}_data data = {{solver_name}}_setup_data(NULL, NULL);
    {{solver_name}}_read_cooling_tables( &data);
    {{solver_name}}_read_rate_tables( &data);
    // set a final runtime
    double dtf = pow(6.67e-8*mh*density, -0.5);
    double z   = 0.0;

    // prepare input data
    double *input = (double *) malloc(sizeof(double)*dims*nchem);
    double *atol  = (double *) malloc(sizeof(double)*dims*nchem);
    double rtol  = 1.0e-5;
    double *temp;
    double dt;
    // initialize ydata

    initialize_long_ydata(input, dims, density, temperature, h2fraction, efraction);
    for (int i = 0; i<nchem; i++)
    {
        printf("input[%d] = %0.5g\n", i, input[i]);
    }
    dengo_evolve_{{solver_name}}(dtf, dt, z, input, &rtol, atol, dims, &data, temp);

    for (int i = 0; i<nchem; i++)
    {
        printf("output[%d] = %0.5g\n", i, input[i]);
    }

    // supposingly this would return the output in "input" array
    double diff;
    for (unsigned long i = 0; i<dims; i++)
    {
        diff = (input[i] - input[i%nchem]) / input[i%nchem];
        if (fabs(diff) > rtol){
            printf("outputi %lu diff[%d] = %0.5g; ref = %0.5g; out = %0.5g\n", i,  i%nchem, diff, input[i], input[i%nchem]);
        }
    }

    return 0;


}

// helper function to lookfor the best configuration for kernels


// cuda performance tuner
// https://developer.nvidia.com/blog/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/
// the idea now is to benchmark the best ratios for rhs_kernel and jacobian_kernel, and interpolation kernel

void launchInterpolationKernel({{solver_name}}_data *data)
{

    // initialize temperature;
    for (int i = 0; i < BATCHSIZE; i++)
    {
        data->Ts[i] = (double) 3000.0 * (i+10)/BATCHSIZE;
        data->logTs[i] = log(data->Ts[i]);
    }

    int blockSize;
    int minGridSize;
    int gridSize;

    // heuristically calculatate a block size that achieves the maximum multiprocessor level occupancy
    cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, linear_interpolation_kernel, 0,0);

    // roundup according to BATCHSIZE

    gridSize = (BATCHSIZE + blockSize -1)/blockSize;
    printf("gridSize = %d; blockSize = %d\n", gridSize, blockSize);
    linear_interpolation_kernel<<<gridSize, blockSize>>>(*data);

    cudaDeviceSynchronize();
    // calculate theoretical occupancy
    int maxActiveBlocks;
    cudaOccupancyMaxActiveBlocksPerMultiprocessor( &maxActiveBlocks,
            linear_interpolation_kernel, blockSize,
            0);

    int device;
    cudaDeviceProp props;
    cudaGetDevice(&device);
    cudaGetDeviceProperties(&props, device);

    float occupancy = (maxActiveBlocks * blockSize / props.warpSize) /
        (float)(props.maxThreadsPerMultiProcessor /
                props.warpSize);

    printf("maxActiveBlocks: %d; blockSize: %d; props.warpSize: %d\n", maxActiveBlocks, blockSize, props.warpSize);
    printf("MaxThreadsperSM: %d\n", props.maxThreadsPerMultiProcessor);
    printf("Launched blocks of size %d. Theoretical occupancy: %f\n",
            blockSize, occupancy);


    /*
       float time;
       cudaEvent_t start, stop;
       cudaEventCreate(&start);
       cudaEventCreate(&stop);
       cudaEventRecord(start, 0);
       for (int j = 0; j < 8192; j++){
       linear_interpolation_kernel<<<GRIDSIZE, BLOCKSIZE>>>(data);
       }
       cudaEventRecord(stop, 0);
       cudaEventSynchronize(stop);
       cudaEventElapsedTime(&time, start, stop);
       printf("Time to generate:  %3.1f ms \n", time);

       cudaDeviceSynchronize();
       cudaError_t cuerr = cudaGetLastError();
       if (cuerr != cudaSuccess) {
       fprintf(stderr,
       ">>> ERROR in interpolation_kernel: cudaGetLastError returned %s\n",
       cudaGetErrorName(cuerr));
       }
     */


}


void launchTemperatureKernel({{solver_name}}_data *data)
{

    int neq = BATCHSIZE*nchem;

    N_Vector y = N_VNew_Cuda(neq);
    double *ydata;
    ydata = N_VGetHostArrayPointer_Cuda(y);
    initialize_ydata(ydata, BATCHSIZE);
    N_VCopyToDevice_Cuda(y);


    ydata = N_VGetDeviceArrayPointer_Cuda(y);

    // launch temp kernel
    int blockSize;
    int minGridSize;
    int gridSize;

    // heuristically calculatate a block size that achieves the maximum multiprocessor level occupancy
    cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, temperature_kernel, 0,0);

    // roundup according to BATCHSIZE
    gridSize = (BATCHSIZE + blockSize -1)/blockSize;
    printf("gridSize = %d; blockSize = %d\n", gridSize, blockSize);
    temperature_kernel<<<gridSize, blockSize>>>(ydata, *data);
    cudaDeviceSynchronize();
    // calculate theoretical occupancy
    int maxActiveBlocks;
    cudaOccupancyMaxActiveBlocksPerMultiprocessor( &maxActiveBlocks,
            temperature_kernel, blockSize,
            0);
    int device;
    cudaDeviceProp props;
    cudaGetDevice(&device);
    cudaGetDeviceProperties(&props, device);
    float occupancy = (maxActiveBlocks * blockSize / props.warpSize) /
        (float)(props.maxThreadsPerMultiProcessor /
                props.warpSize);

    printf("maxActiveBlocks: %d; blockSize: %d; props.warpSize: %d\n", maxActiveBlocks, blockSize, props.warpSize);
    printf("MaxThreadsperSM: %d\n", props.maxThreadsPerMultiProcessor);
    printf("Launched temperature Kernel blocks of size %d. Theoretical occupancy: %f\n",
            blockSize, occupancy);

    // end launch temp kernel

}

void launchRhsKernel({{solver_name}}_data *data)
{

    double t = 1.0;
    int neq = BATCHSIZE*nchem;
    printf("done reading data %d\n", neq);

    N_Vector y    = N_VNew_Cuda(neq);
    N_Vector ydot = N_VNew_Cuda(neq);
    double *ydata, *ydotdata;
    ydata    = N_VGetHostArrayPointer_Cuda(y);
    ydotdata = N_VGetHostArrayPointer_Cuda(ydot);
    initialize_ydata(ydata, BATCHSIZE);
    initialize_ydata(ydotdata, BATCHSIZE);
    N_VCopyToDevice_Cuda(y);
    N_VCopyToDevice_Cuda(ydot);

    double *ydata_dev    = N_VGetDeviceArrayPointer_Cuda(y);
    double *ydotdata_dev = N_VGetDeviceArrayPointer_Cuda(ydot);
    // launch temp kernel
    int blockSize;
    int minGridSize;
    int gridSize;

    // heuristically calculatate a block size that achieves the maximum multiprocessor level occupancy
    cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, rhs_kernel, 0,0);

    // roundup according to BATCHSIZE
    gridSize = (BATCHSIZE + blockSize -1)/blockSize;
    printf("gridSize = %d; blockSize = %d\n", gridSize, blockSize);
    rhs_kernel<<<gridSize, blockSize>>>(t,ydata_dev, ydotdata_dev, *data);
    cudaDeviceSynchronize();

    cudaError_t cuerr = cudaGetLastError();
    if (cuerr != cudaSuccess) {
        fprintf(stderr,
                ">>> ERROR in f: cudaGetLastError returned %s\n",
                cudaGetErrorName(cuerr));
    }
    // calculate theoretical occupancy
    int maxActiveBlocks;
    cudaOccupancyMaxActiveBlocksPerMultiprocessor( &maxActiveBlocks,
            rhs_kernel, blockSize,
            0);
    int device;
    cudaDeviceProp props;
    cudaGetDevice(&device);
    cudaGetDeviceProperties(&props, device);
    float occupancy = (maxActiveBlocks * blockSize / props.warpSize) /
        (float)(props.maxThreadsPerMultiProcessor /
                props.warpSize);
    printf("maxActiveBlocks: %d; blockSize: %d; props.warpSize: %d\n", maxActiveBlocks, blockSize, props.warpSize);
    printf("MaxThreadsperSM: %d\n", props.maxThreadsPerMultiProcessor);

    printf("Launched Rhs Kernel blocks of size %d. Theoretical occupancy: %f\n",
            blockSize, occupancy);
    cudaDeviceSynchronize();
}


void launchJacobianKernel({{solver_name}}_data *data)

{

    int neq = BATCHSIZE*nchem;

    N_Vector y = N_VNew_Cuda(neq);
    double *ydata, *Jdata;
    ydata = N_VGetHostArrayPointer_Cuda(y);
    initialize_ydata(ydata, BATCHSIZE);
    N_VCopyToDevice_Cuda(y);

    ydata = N_VGetDeviceArrayPointer_Cuda(y);
    N_Vector ydot = N_VNew_Cuda(neq);

    printf("done preparing data\n");
    // also need to initialize jacobian data space

    /* Create sparse SUNMatrix for use in linear solves */
    SUNMatrix A;
    A = NULL;
    cusparseHandle_t cusp_handle;
    cusparseCreate(&cusp_handle);
    A = SUNMatrix_cuSparse_NewBlockCSR(BATCHSIZE, nchem, nchem, nchem*nchem, cusp_handle);
    /* Initialiize the Jacobian with its fixed sparsity pattern */
    blockJacInit(A);

    Jdata = SUNMatrix_cuSparse_Data(A);

    printf("just before launching\n");
    // launch temp kernel
    int blockSize;
    int minGridSize;
    int gridSize;

    // heuristically calculatate a block size that achieves the maximum multiprocessor level occupancy
    cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, jacobian_kernel, 0,0);

    // roundup according to BATCHSIZE
    gridSize = (BATCHSIZE + blockSize -1)/blockSize;
    printf("gridSize = %d; blockSize = %d\n", gridSize, blockSize);
    jacobian_kernel<<<gridSize, blockSize>>>(ydata, Jdata, *data);
    cudaDeviceSynchronize();
    // calculate theoretical occupancy
    int maxActiveBlocks;
    cudaOccupancyMaxActiveBlocksPerMultiprocessor( &maxActiveBlocks,
            jacobian_kernel, blockSize,
            0);
    cudaError_t cuerr = cudaGetLastError();
    if (cuerr != cudaSuccess) {
        fprintf(stderr, ">>> ERROR in MaxActiveBlocks: cudaGetLastError returned %s\n",
                cudaGetErrorName(cuerr));
    }
    int device;
    cudaDeviceProp props;
    cudaGetDevice(&device);
    cudaGetDeviceProperties(&props, device);
    float occupancy = (maxActiveBlocks * blockSize / props.warpSize) /
        (float)(props.maxThreadsPerMultiProcessor /
                props.warpSize);
    printf("maxActiveBlocks: %d; blockSize: %d; props.warpSize: %d\n", maxActiveBlocks, blockSize, props.warpSize);
    printf("MaxThreadsperSM: %d\n", props.maxThreadsPerMultiProcessor);

    printf("Launched Jacobian Kernel blocks of size %d. Theoretical occupancy: %f\n",
            blockSize, occupancy);
}
